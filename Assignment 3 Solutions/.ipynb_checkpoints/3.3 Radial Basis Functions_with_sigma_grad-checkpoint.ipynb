{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radial Basis Functions\n",
    "\n",
    "Radial Basis Function Networks are a simpler, modified version of Multi Layer Perceptrons with the following modifications:\n",
    "* The network only has three layers, an input, a hidden and an output layer\n",
    "* The input layer has neurons equal to the dimensionality of the training data. Since we're playing around with points in the 2D Cartesian plane as inputs, our input layer will only have two neurons \n",
    "* The major difference lies in the hidden layer. Though this layer may contain any number of neurons, these have to use a _radial_ activation function (and hence the name RBF Networks). These functions look like something of the form $$ \\phi(x) = e^{-\\beta || x - \\mu ||^2 } $$ where $x$ is the input and $\\mu$ is the _center_ of the RBF neuron. $\\beta$ is a hyperparameter.\n",
    "* For classification problems like ours, the output layer, as usual, is typically equal to the number of classes.\n",
    "* The general notion of weights and biases is redefined here. RBF Nets generally don't have weights from the input to the hidden layer, and typically don't have a bias term.\n",
    "\n",
    "There are a variety of different ways to train an RBF Network, but typically we only train the weights from the hidden layer to the output layer. Other parameters like the _centers_ of the RBF neurons and $\\beta$s are trained in a separate manner. In this tutorial, we only plan to learn the weights of the RBF Network. One should also note that gradient descent is not the most obvious choice for training RBF Nets, so resources on the topic might be scarce.\n",
    "\n",
    "* For a more complete understanding of RBF Networks, see http://mccormickml.com/2013/08/15/radial-basis-function-network-rbfn-tutorial/\n",
    "* To know more about the different ways to train an RBF Net, see http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.109.312&rep=rep1&type=pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jdc in /home/nbcommon/anaconda3_410/lib/python3.5/site-packages\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# To split a class across multiple cells\n",
    "pip3 install jdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import random\n",
    "import numpy as np\n",
    "import jdc\n",
    "from datasets import *\n",
    "# For some handy helper functions\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue using the Network class similar to the assignment on Multi Layer Perceptrons\n",
    "\n",
    "**Note, again:** We are using jdc to define each method of `class Network` in seperate cells. jdc follows the following syntax,\n",
    "\n",
    "```py\n",
    "%%add_to #CLASS_NAME#\n",
    "def dummy_method(self):\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes, training_data):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network. For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.\"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.initialize_biases()\n",
    "        self.initialize_weights()\n",
    "        self.initialize_gaussians()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "## 3.3.1 Initialize weights and centers\n",
    "\n",
    "The weights for the network are initialized randomly from a Gaussian distribution with mean 0, and variance 1. Note that we only have weights between the hidden and output layers. The centers, however, are randomly selected out of the training points. Implement the following function to initialize weights and centers.\n",
    "\n",
    "![image.png](https://chrisjmccormick.files.wordpress.com/2013/08/architecture_simple2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def initialize_biases(self):\n",
    "    self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def initialize_weights(self):\n",
    "    self.weights = [np.random.randn(y, x)\n",
    "                    for x, y in zip(self.sizes[:-1], self.sizes[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def initialize_gaussians(self):\n",
    "    # Initialize centers\n",
    "    # We assume that the points are normalized\n",
    "    # So centers should be between -1 and 1\n",
    "    self.centers = [np.random.randn(y, x) \n",
    "                    for x, y in zip(self.sizes[:-2], self.sizes[1:-1])]\n",
    "    # Initalize radii \n",
    "    self.radii = [np.random.randn(y, x) \n",
    "                    for x, y in zip(self.sizes[:-2], self.sizes[1:-1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "We shall implement backpropogration with stocastic mini-batch gradient descent to optimize out network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def train(self, training_data, epochs, mini_batch_size, learning_rate):\n",
    "    \"\"\"Train the neural network using gradient descent.  \n",
    "    The ``training_data`` is a list of tuples\n",
    "    ``(x, y)`` representing the training inputs and the desired\n",
    "    outputs.  The other parameters are self-explanatory.\"\"\"\n",
    "\n",
    "    # training_data is a list and is passed by reference\n",
    "    # To prevernt affecting the original data we use \n",
    "    # this hack to create a copy of training_data     \n",
    "    training_data = list(training_data)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        # Get mini-batches    \n",
    "        mini_batches = self.create_mini_batches(training_data, mini_batch_size)\n",
    "        \n",
    "        # Itterate over mini-batches to update pramaters   \n",
    "        cost = sum(map(lambda mini_batch: self.update_params(mini_batch, learning_rate), mini_batches))\n",
    "        \n",
    "        # Find accuracy of the model at the end of epoch         \n",
    "        acc = self.evaluate(training_data)\n",
    "        \n",
    "        print(\"Epoch {} complete. Total cost: {}, Accuracy: {}\".format(i, cost, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.2 Create mini-batches\n",
    "\n",
    "Split the training data into mini-batches of size `mini_batch_size` and return a list of mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def create_mini_batches(self, training_data, mini_batch_size):\n",
    "    # Shuffling data helps a lot in mini-batch SDG\n",
    "    random.shuffle(training_data)\n",
    "    mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, len(training_data), mini_batch_size)]\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.3 Update weights and centers\n",
    " The update rules for centers and weights respectively, are:\n",
    "\n",
    "$$ c_{j} (t+1) = c_{j} (t) - \\eta_{1} \\frac{\\partial E}{\\partial c_{j}} $$\n",
    "$$ w_{i} (t+1) = w_{i} (t) - \\eta_{2} \\frac{\\partial E}{\\partial w_{i}} $$\n",
    "\n",
    "The cost function, as usual, is $ E = \\frac{1}{2} \\sum (y^d - y)^2 $ where $y^d$ is the target output and $y$ is the predicted output, which is again written as $ y = \\sum_{i=1}^{h} \\phi_i w_i $\n",
    "\n",
    "$\\phi_i$ is the radial basis function being used. $ \\phi_i = e^{-z_i^2 / 2 \\sigma^2} $ where $z_i = || x - c_i|| $ is the Euclidian distance between $x$ and $c_i$ and $\\sigma$ is the width of the center.\n",
    "\n",
    "Differentiating $E$ w.r.t $w_i$, and further simpifying, we get \n",
    "$$ c_{j} (t+1) = c_{j} (t) + \\eta_{1} (y^d - y) w_i \\frac{\\phi_i}{\\sigma^2} (x_j - c_j) $$\n",
    "Similarly differentiating $E$ w.r.t $c_i$, and further simpifying, we get\n",
    "$$ w_{i} (t+1) = w_{i} (t) + \\eta_{2} (y^d - y) \\phi_i $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def update_params(self, mini_batch, learning_rate):\n",
    "    \"\"\"Update the network's weights and biases by applying\n",
    "    gradient descent using backpropagation.\"\"\"\n",
    "    \n",
    "    # Initialize gradients     \n",
    "    delta_b = [np.zeros(b.shape) for b in self.biases]\n",
    "    delta_w = [np.zeros(w.shape) for w in self.weights]\n",
    "    delta_c = [np.zeros(c.shape) for c in self.centers]\n",
    "    delta_r = [np.zeros(r.shape) for r in self.radii]\n",
    "    \n",
    "    \n",
    "    total_cost = 0\n",
    "    \n",
    "    for x, y in mini_batch:\n",
    "        # Obtain the mean squered error and the gradeinets\n",
    "        # with resepect to biases and weights        \n",
    "        cost, del_b, del_w, del_c, del_r = self.backprop(x, y)\n",
    "        \n",
    "        # Add the gradients for each sample in mini-batch\n",
    "        delta_b = [nb + dnb for nb, dnb in zip(delta_b, del_b)]\n",
    "        delta_w = [nw + dnw for nw, dnw in zip(delta_w, del_w)]\n",
    "        delta_c = [nc + dnc for nc, dnc in zip(delta_c, del_c)]\n",
    "        delta_r = [nr + dnr for nr, dnr in zip(delta_r, del_r)]\n",
    "        \n",
    "        total_cost += cost\n",
    "\n",
    "    # Update self.biases and self.weights\n",
    "    # using delta_b, delta_w and learning_rate        \n",
    "    self.biases = [b - (learning_rate / len(mini_batch)) * db\n",
    "                   for b, db in zip(self.biases, delta_b)]\n",
    "    self.weights = [w - (learning_rate / len(mini_batch)) * dw\n",
    "                    for w, dw in zip(self.weights, delta_w)]\n",
    "    self.centers = [c - (learning_rate / len(mini_batch)) * dc\n",
    "                   for c, dc in zip(self.centers, delta_c)]\n",
    "    self.radii = [r - (learning_rate / len(mini_batch)) * dr\n",
    "                    for r, dr in zip(self.radii, delta_r)]\n",
    "    \n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def backprop(self, x, y):\n",
    "    \"\"\"Return arry containiing cost, del_b, del_w representing the\n",
    "    cost function C(x) and gradient for cost function.  ``del_b`` and\n",
    "    ``del_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "    to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "    # Forward pass\n",
    "    zs, numerators, fractions, activations = self.forward(x)\n",
    "    \n",
    "    # Backward pass     \n",
    "    cost, del_b, del_w, del_c, del_r = self.backward(zs, numerators, fractions, activations, y)\n",
    "    \n",
    "    return cost, del_b, del_w, del_c, del_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.5 Implement functions to calculate sigmoid and it's derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.6 Implement forward propogration\n",
    "\n",
    "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vRu0-T4iRYiif9UAVgEia-fLPd2c0FB4EOO_AiQzLGAU1gBadvscWUpKMG533PTSTXVqcagukcbHOK3/pub?w=656&amp;h=370\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def gaussian(self, z, c, r):\n",
    "    numerator = z - c\n",
    "    fraction = numerator / r\n",
    "    exponent = -np.sum(((fraction ** 2) / 2), axis=1).reshape(-1, 1)\n",
    "    gaussian = np.exp(exponent)\n",
    "    \n",
    "    return numerator, fraction, gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def gaussian_derivative(self, numerator, fraction, gaussian, z, c, r):\n",
    "    # gaussian = exp(exponent)\n",
    "    dexponent = gaussian\n",
    "    \n",
    "    # inverting the summation op\n",
    "    print(gaussian.shape)\n",
    "    print(z.shape)\n",
    "    dexponent = np.hstack([gaussian] * z.shape[2])\n",
    "    \n",
    "    # exponent = -1/2 * fraction_square\n",
    "    dfraction_square = (-1/2) * dexponent\n",
    "    \n",
    "    # fraction_square = fraction ** 2\n",
    "    dfraction = 2 * fraction * dexponent\n",
    "\n",
    "    # fraction = numerator * denominator\n",
    "    # denominator = 1/r     \n",
    "    dnumerator = (1/r) * dfraction\n",
    "    ddenominator = numerator * dfraction\n",
    "\n",
    "    # denominator = 1 / r\n",
    "    dr =(-1.0 / (r ** 2)) * ddenominator\n",
    "    \n",
    "    # numerator = z - c\n",
    "    dz = dnumerator\n",
    "    dc = -dnumerator\n",
    "\n",
    "    return dz, dc, dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def forward(self, x):\n",
    "    \"\"\"Compute Z and actiavtion for each layer.\"\"\"\n",
    "    \n",
    "    # list to store all the activations, layer by layer\n",
    "    zs, numerators, fractions, activations = [[] for _ in range(4)]\n",
    "    \n",
    "    activations.append(x)\n",
    "    \n",
    "    # Loop through each layer to compute activations and Zs    \n",
    "    for i in range(len(self.biases)):\n",
    "        # Fetch weights and biases         \n",
    "        b, w = self.biases[i], self.weights[i]\n",
    "        \n",
    "        # Calculate activation\n",
    "        if i == len(self.biases) - 1:\n",
    "            z = np.dot(w, activations[-1]) + b\n",
    "            # No activation in last layer             \n",
    "            activations.append(z)\n",
    "        else:\n",
    "            z = w.transpose() * activations[-1] + b.transpose()\n",
    "    \n",
    "            c, r = self.centers[i], self.radii[i]\n",
    "            numerator, fraction, gaussian = self.gaussian(z, c, r)\n",
    "            \n",
    "            zs.append(z)\n",
    "            numerators.append(numerator)\n",
    "            fractions.append(fraction)\n",
    "            activations.append(gaussian)\n",
    "        \n",
    "    return zs, numerators, fractions, activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.7 Implement functions to calculate mean squared error and  it's derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def mse(self, output_activations, y):\n",
    "    \"\"\"Returns mean square error.\"\"\"\n",
    "    return sum((output_activations - y) ** 2 / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def mse_derivative(self, output_activations, y):\n",
    "    \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "    \\partial a for the output activations. \"\"\"\n",
    "    return output_activations - y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.8 Implement backward pass\n",
    "\n",
    "Refer [this](http://colah.github.io/posts/2015-08-Backprop/) blog by Christopher Olah to understand how gradient commpuation happens during backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<string>, line 29)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/nbuser/anaconda3_410/lib/python3.5/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2862\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-16-ebf41610c204>\"\u001b[0m, line \u001b[1;32m1\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    get_ipython().run_cell_magic('add_to', 'Network', 'def backward(self, zs, numerators, fractions, activations, y):\\n    \"\"\"Compute and return cost funcation, gradients for \\n    weights and biases for each layer.\"\"\"\\n    # Initialize gradient arrays\\n    del_b = [np.zeros(b.shape) for b in self.biases]\\n    del_w = [np.zeros(w.shape) for w in self.weights]\\n    del_c = [np.zeros(c.shape) for c in self.centers]\\n    del_r = [np.zeros(r.shape) for r in self.radii]\\n    \\n    # Compute for last layer\\n    cost = self.mse(activations[-1], y)\\n    delta = self.mse_derivative(activations[-1], y)\\n    \\n    del_b[-1] = delta\\n    \\n    del_w[-1] = np.dot(delta, activations[-2].transpose())\\n    \\n    # Loop through each layer in reverse direction to \\n    # populate del_b and del_w   \\n    for l in range(2, self.num_layers):\\n        dz, dc, dr = self.gaussian_derivative(numerators[-l + 1], fractions[-l + 1], \\n                                              activations[-l],  zs[-l + 1], \\n                                              self.centers[-l + 1], self.radii[-l + 1])\\n        del_c[-l + 1] = dc\\n        del_r[-l + 1] = dr\\n        if l == 2:\\n            delta = np.dot(self.weights[-l + 1].transpose(), delta) * dz\\n        else:\\n            delta = \\n        del_b[-l] = delta\\n        del_w[-l] = np.dot(delta, activations[-l - 1].transpose())\\n    return cost, del_b, del_w, del_c, del_r')\n",
      "  File \u001b[1;32m\"/home/nbuser/anaconda3_410/lib/python3.5/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2103\u001b[0m, in \u001b[1;35mrun_cell_magic\u001b[0m\n    result = fn(magic_arg_s, cell)\n",
      "  File \u001b[1;32m\"<decorator-gen-125>\"\u001b[0m, line \u001b[1;32m2\u001b[0m, in \u001b[1;35madd_to\u001b[0m\n",
      "  File \u001b[1;32m\"/home/nbuser/anaconda3_410/lib/python3.5/site-packages/IPython/core/magic.py\"\u001b[0m, line \u001b[1;32m187\u001b[0m, in \u001b[1;35m<lambda>\u001b[0m\n    call = lambda f, *a, **k: f(*a, **k)\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/nbuser/anaconda3_410/lib/python3.5/site-packages/jdc/__init__.py\"\u001b[0;36m, line \u001b[0;32m35\u001b[0;36m, in \u001b[0;35madd_to\u001b[0;36m\u001b[0m\n\u001b[0;31m    exec(run_str, lcls)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32m29\u001b[0m\n\u001b[0;31m    delta =\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%%add_to Network\n",
    "def backward(self, zs, numerators, fractions, activations, y):\n",
    "    \"\"\"Compute and return cost funcation, gradients for \n",
    "    weights and biases for each layer.\"\"\"\n",
    "    # Initialize gradient arrays\n",
    "    del_b = [np.zeros(b.shape) for b in self.biases]\n",
    "    del_w = [np.zeros(w.shape) for w in self.weights]\n",
    "    del_c = [np.zeros(c.shape) for c in self.centers]\n",
    "    del_r = [np.zeros(r.shape) for r in self.radii]\n",
    "    \n",
    "    # Compute for last layer\n",
    "    cost = self.mse(activations[-1], y)\n",
    "    delta = self.mse_derivative(activations[-1], y)\n",
    "    \n",
    "    del_b[-1] = delta\n",
    "    \n",
    "    del_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "    \n",
    "    # Loop through each layer in reverse direction to \n",
    "    # populate del_b and del_w   \n",
    "    for l in range(2, self.num_layers):\n",
    "        dz, dc, dr = self.gaussian_derivative(numerators[-l + 1], fractions[-l + 1], \n",
    "                                              activations[-l],  zs[-l + 1], \n",
    "                                              self.centers[-l + 1], self.radii[-l + 1])\n",
    "        del_c[-l + 1] = dc\n",
    "        del_r[-l + 1] = dr\n",
    "        if l == 2:\n",
    "            delta = np.dot(self.weights[-l + 1].transpose(), delta) * dz\n",
    "        else: \n",
    "        del_b[-l] = delta\n",
    "        del_w[-l] = np.dot(delta, activations[-l - 1].transpose())\n",
    "    return cost, del_b, del_w, del_c, del_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def evaluate(self, test_data):\n",
    "    \"\"\"Return the accuracy of Network. Note that the neural\n",
    "    network's output is assumed to be the index of whichever\n",
    "    neuron in the final layer has the highest activation.\"\"\"\n",
    "    test_results = [(np.argmax(self.forward(x)[0]), np.argmax(y))\n",
    "                    for (x, y) in test_data]\n",
    "    return sum(int(x == y) for (x, y) in test_results) * 100 / len(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showtime\n",
    "\n",
    "Let's test our implementation on a bunch of datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = generate_datsets()\n",
    "\n",
    "plot_datasets(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_process_data(x, y):\n",
    "    # Find number samples     \n",
    "    n = len(y)\n",
    "    # Find number classes\n",
    "    c  = max(y) + 1\n",
    "    \n",
    "    # Normalize the input\n",
    "    x = sklearn.preprocessing.normalize(x)\n",
    "    \n",
    "    x = np.split(x, n)\n",
    "    x = [a.reshape(-1, 1) for a in x]\n",
    "    \n",
    "    # Convert lables to one-hot vectors     \n",
    "    one_hot = np.zeros([n, c])\n",
    "    one_hot[range(n), y] = 1\n",
    "    \n",
    "    y = np.split(one_hot, n)\n",
    "    y = [a.reshape(-1, 1) for a in y]\n",
    "    \n",
    "    return list(zip(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.9 Try out different hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasets_with_pred = {}\n",
    "\n",
    "for name, dataset in datasets.items():\n",
    "    \n",
    "    print(\"Training dataset: {}\".format(name))\n",
    "\n",
    "    X, Y = dataset\n",
    "\n",
    "    # Find number classes\n",
    "    c  = max(Y) + 1\n",
    "    \n",
    "    # Pre-process the data\n",
    "    # Checkout the implementation for some cool numpy tricks     \n",
    "    training_data = pre_process_data(X, Y)\n",
    "        \n",
    "    network = Network([2, 7, c], training_data)\n",
    "    \n",
    "    network.train(training_data, 30, 10, 0.4)\n",
    "    \n",
    "    predictions = list(map(lambda sample: np.argmax(network.forward(sample[0])[0]), training_data))\n",
    "   \n",
    "    datasets_with_pred[name] = X, Y, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_datasets(datasets_with_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
